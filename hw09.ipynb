{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "hw09_task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqbO7WycYLcK"
      },
      "source": [
        "# Случайные леса\n",
        "__Суммарное количество баллов: 10__\n",
        "\n",
        "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
        "\n",
        "__Тема письма: `[ITMO][ML][MS][HW09] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
        "\n",
        "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
        "\n",
        "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL37o372YLcK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import copy\n",
        "#from catboost import CatBoostClassifier"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-nYWo-9YLcK"
      },
      "source": [
        "def gini(x):\n",
        "    _, counts = np.unique(x, return_counts=True)\n",
        "    proba = counts / len(x)\n",
        "    return np.sum(proba * (1 - proba))\n",
        "    \n",
        "def entropy(x):\n",
        "    _, counts = np.unique(x, return_counts=True)\n",
        "    proba = counts / len(x)\n",
        "    return -np.sum(proba * np.log2(proba))\n",
        "\n",
        "def gain(left_y, right_y, criterion):\n",
        "    y = np.concatenate((left_y, right_y))\n",
        "    return criterion(y) - (criterion(left_y) * len(left_y) + criterion(right_y) * len(right_y)) / len(y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8rT5NXTYLcK"
      },
      "source": [
        "### Задание 1 (2 балла)\n",
        "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
        "\n",
        "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
        "\n",
        "#### Методы\n",
        "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
        "\n",
        "#### Параметры конструктора\n",
        "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
        "\n",
        "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
        "\n",
        "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
        "\n",
        "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
        "\n",
        "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXPXMsHPYLcK"
      },
      "source": [
        "class DecisionTreeLeaf:\n",
        "    \"\"\"\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    y : Тип метки (напр., int или str)\n",
        "        Метка класса, который встречается чаще всего среди элементов листа дерева\n",
        "    \"\"\"\n",
        "    def __init__(self, y):\n",
        "      classes, counts = np.unique(y, return_counts=True)\n",
        "      self.y = classes[np.argmax(counts)]\n",
        "      self.proba = dict(zip(classes, counts / np.sum(counts)))#...\n",
        "\n",
        "    def predict(self, X):\n",
        "      return np.full(len(X), self.proba)\n",
        "        \n",
        "\n",
        "class DecisionTreeNode:\n",
        "    \"\"\"\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    split_dim : int\n",
        "        Измерение, по которому разбиваем выборку.\n",
        "    split_value : float\n",
        "        Значение, по которому разбираем выборку.\n",
        "    left : Union[DecisionTreeNode, DecisionTreeLeaf]\n",
        "        Поддерево, отвечающее за случай x[split_dim] < split_value.\n",
        "    right : Union[DecisionTreeNode, DecisionTreeLeaf]\n",
        "        Поддерево, отвечающее за случай x[split_dim] >= split_value. \n",
        "    \"\"\"\n",
        "    def __init__(self, split_dim: int,\n",
        "                 left, right):\n",
        "        self.split_dim = split_dim\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        \n",
        "    def predict(self, X):\n",
        "        y = len(X) * [None]\n",
        "        left_mask = X[:, self.split_dim] == 0\n",
        "        right_mask = X[:, self.split_dim] == 1\n",
        "        left_pred = self.left.predict(X[left_mask])\n",
        "        right_pred = self.right.predict(X[right_mask])\n",
        "        left  = 0\n",
        "        right = 0\n",
        "        for i in range(len(y)):\n",
        "            if left_mask[i]:\n",
        "                y[i] = left_pred[left]\n",
        "                left += 1\n",
        "            else:\n",
        "                y[i] = right_pred[right]\n",
        "                right += 1\n",
        "        return y\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, X, y, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\"):\n",
        "        if criterion == \"gini\":\n",
        "          self.criterion = gini\n",
        "        else:\n",
        "          self.criterion = entropy\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        if max_features == \"auto\":\n",
        "          self.max_features = int(np.sqrt(X.shape[1]))\n",
        "        else: \n",
        "          self.max_features = max_features\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _find_best_split(self, X, y): \n",
        "        best_gain = -1\n",
        "        best_split_by = None\n",
        "        dims = np.random.choice(X.shape[1], self.max_features)\n",
        "        for dim in dims:\n",
        "          left_y = y[X[:, dim] == 0]\n",
        "          if (len(left_y) < self.min_samples_leaf) or (len(y) - len(left_y) < self.min_samples_leaf):\n",
        "                    continue\n",
        "          right_y = y[X[:, dim] == 1]\n",
        "          score = gain(left_y, right_y, self.criterion)\n",
        "          if score > best_gain:\n",
        "            best_gain = score\n",
        "            best_split_by = dim\n",
        "        return best_split_by      \n",
        "      \n",
        "    def _build_tree(self,X, y, depth = 0):\n",
        "            if self.max_depth is not None and depth >= self.max_depth:\n",
        "            #if depth >= self.max_depth and self.max_depth is not None or X.shape[0] <= self.min_samples:\n",
        "              return DecisionTreeLeaf(y)\n",
        "            best_split_by = self._find_best_split(X, y)\n",
        "            if best_split_by is None:\n",
        "                return DecisionTreeLeaf(y)               \n",
        "            left_mask = X[:, best_split_by] == 0\n",
        "            right_mask = X[:, best_split_by] == 1\n",
        "            return DecisionTreeNode(best_split_by,\n",
        "                                    self._build_tree(X[left_mask], y[left_mask], depth+1),\n",
        "                                    self._build_tree(X[right_mask], y[right_mask], depth+1))      \n",
        "     \n",
        "    \n",
        "    def predict_proba(self, X: np.ndarray):\n",
        "        return self.root.predict(X)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return [max(p.keys(), key=lambda k: p[k]) for p in proba] "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2KmPLLsYLcK"
      },
      "source": [
        "### Задание 2 (2 балла)\n",
        "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
        "\n",
        "#### Параметры конструктора\n",
        "`n_estimators` - количество используемых для предсказания деревьев.\n",
        "\n",
        "Остальное - параметры деревьев.\n",
        "\n",
        "#### Методы\n",
        "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
        "\n",
        "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7FPvlQZYLcK"
      },
      "source": [
        "class RandomForestClassifier:\n",
        "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.n_estimators = n_estimators\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        size = int(X.shape[0] / self.n_estimators)\n",
        "        indices = np.arange(X.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        self.indices = indices\n",
        "        for i in range(self.n_estimators):\n",
        "            self.trees.append(DecisionTree(X[indices[size*i:size*(i+1)]], y[indices[size*i:size*(i+1)]], self.criterion, self.max_depth, self.min_samples_leaf, self.max_features))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        pred = []\n",
        "        res = []\n",
        "        for tree in self.trees:\n",
        "            pred.append(tree.predict(X))\n",
        "        pred = np.array(pred)\n",
        "        for i in range(X.shape[0]):\n",
        "            classes, counts = np.unique(pred[:, i], return_counts=True)\n",
        "            res.append(classes[np.argmax(counts)])\n",
        "        return np.array(res)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M29Kwtc6YLcK"
      },
      "source": [
        "### Задание 3 (2 балла)\n",
        "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
        "\n",
        "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFScwu5fYLcK"
      },
      "source": [
        "def feature_importance(rfc):\n",
        "  X = rfc.X\n",
        "  y = rfc.y \n",
        "  indices = rfc.indices\n",
        "  importance = [None]*X.shape[1]\n",
        "  size = int(X.shape[0] / rfc.n_estimators)  \n",
        "  for i, tree in enumerate(rfc.trees):\n",
        "    oob_ind = np.concatenate((indices[:i*size], indices[(i+1)*size:]))\n",
        "    err_oob = 1 - np.mean(tree.predict(X[oob_ind]) == y[oob_ind])\n",
        "    for f in range(X.shape[1]):\n",
        "      copy = np.copy(X[oob_ind])\n",
        "      np.random.shuffle(copy[ : , f])\n",
        "      err_oob_j = 1 - np.mean(tree.predict(copy) == y[oob_ind])\n",
        "      err = err_oob_j - err_oob\n",
        "      if importance[f] is None:\n",
        "        importance[f] = err\n",
        "      else:\n",
        "        importance[f] = np.mean([importance[f], err])\n",
        "  return np.array(importance)\n",
        "\n",
        "def most_important_features(importance, names, k=20):\n",
        "    # Выводит названия k самых важных признаков\n",
        "    idicies = np.argsort(importance)[::-1][:k]\n",
        "    return np.array(names)[idicies]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSWhRCZjYLcK"
      },
      "source": [
        "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате точность должна быть примерно равна `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHdRojBwYLcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288874d6-816c-4d48-f23b-49c7095abde1"
      },
      "source": [
        "def synthetic_dataset(size):\n",
        "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
        "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
        "    y = [i % 3 for i in range(size)]\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = synthetic_dataset(1000)\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(X, y)\n",
        "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
        "print(\"Importance:\", feature_importance(rfc))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Importance: [-0.00179771 -0.00036658  0.04129203  0.00226927  0.24442893  0.0038284 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S73oQNN_YLcK"
      },
      "source": [
        "### Задание 4 (1 балл)\n",
        "Теперь поработаем с реальными данными.\n",
        "\n",
        "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
        "\\\n",
        "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
        "\\\n",
        "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dUwc5vNYLcL"
      },
      "source": [
        "def read_dataset(path):\n",
        "    dataframe = pandas.read_csv(path, header=0)\n",
        "    dataset = dataframe.values.tolist()\n",
        "    random.shuffle(dataset)\n",
        "    y_age = [row[0] for row in dataset]\n",
        "    y_sex = [row[1] for row in dataset]\n",
        "    X = [row[2:] for row in dataset]\n",
        "    \n",
        "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqyLWxnzYLcL"
      },
      "source": [
        "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
        "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LSwY5ptYLcL"
      },
      "source": [
        "#### Возраст"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5iV5wjPYLcL"
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=10)\n",
        "\n",
        "rfc.fit(X_train, y_age_train)\n",
        "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
        "    print(str(i+1) + \".\", name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3N5bf3jYLcL"
      },
      "source": [
        "#### Пол"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLYXLNqXYLcL"
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=10)\n",
        "rfc.fit(X_train, y_sex_train)\n",
        "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
        "    print(str(i+1) + \".\", name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47rCXp3EYLcL"
      },
      "source": [
        "### CatBoost\n",
        "В качестве аьтернативы попробуем CatBoost. \n",
        "\n",
        "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
        "\\\n",
        "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl1_AgZiYLcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d02b8c3-3a6b-4e77-f4bf-76aa0d3df44d"
      },
      "source": [
        "#!pip install catboost\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "X, y = synthetic_dataset(1000)\n",
        "model = CatBoostClassifier(iterations=15, learning_rate=0.01, depth=10, loss_function='MultiClass')\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X).reshape(-1,)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", np.mean(y_pred == y))\n",
        "print(\"Importance: \\n\", model.get_feature_importance(prettified=True))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 1.0800326\ttotal: 771us\tremaining: 10.8ms\n",
            "1:\tlearn: 1.0613431\ttotal: 2.22ms\tremaining: 14.4ms\n",
            "2:\tlearn: 1.0437625\ttotal: 3.15ms\tremaining: 12.6ms\n",
            "3:\tlearn: 1.0266410\ttotal: 3.89ms\tremaining: 10.7ms\n",
            "4:\tlearn: 1.0094088\ttotal: 4.53ms\tremaining: 9.06ms\n",
            "5:\tlearn: 0.9926312\ttotal: 5.19ms\tremaining: 7.78ms\n",
            "6:\tlearn: 0.9762891\ttotal: 5.83ms\tremaining: 6.66ms\n",
            "7:\tlearn: 0.9608643\ttotal: 6.57ms\tremaining: 5.75ms\n",
            "8:\tlearn: 0.9458134\ttotal: 7.37ms\tremaining: 4.92ms\n",
            "9:\tlearn: 0.9306501\ttotal: 8.11ms\tremaining: 4.05ms\n",
            "10:\tlearn: 0.9163152\ttotal: 8.86ms\tremaining: 3.22ms\n",
            "11:\tlearn: 0.9018678\ttotal: 9.48ms\tremaining: 2.37ms\n",
            "12:\tlearn: 0.8881964\ttotal: 10.2ms\tremaining: 1.57ms\n",
            "13:\tlearn: 0.8744128\ttotal: 10.8ms\tremaining: 774us\n",
            "14:\tlearn: 0.8613576\ttotal: 11.6ms\tremaining: 0us\n",
            "Accuracy: 1.0\n",
            "Importance: \n",
            "   Feature Id  Importances\n",
            "0          4    43.618934\n",
            "1          3    28.198692\n",
            "2          2    28.181315\n",
            "3          5     0.000785\n",
            "4          1     0.000182\n",
            "5          0     0.000092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP90x4dHYLcL"
      },
      "source": [
        "### Задание 5 (3 балла)\n",
        "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
        "\\\n",
        "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BPWqyt2YLcL"
      },
      "source": [
        "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
        "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
        "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uiiOCKPYLcL"
      },
      "source": [
        "#### Возраст"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuonVYflYLcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985e01ef-0d53-4419-b623-04b872822228"
      },
      "source": [
        "model = CatBoostClassifier(iterations=15, learning_rate=0.01, depth=10, loss_function='MultiClass')\n",
        "model.fit(X_train, y_age_train)\n",
        "y_pred = model.predict(X_test).reshape(-1,)\n",
        "print(\"Accuracy:\", np.mean(y_pred == y_age_test))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(model.get_feature_importance(), features, 10)):\n",
        "    print(str(i+1) + \".\", name)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 1.0948951\ttotal: 308ms\tremaining: 4.31s\n",
            "1:\tlearn: 1.0911074\ttotal: 569ms\tremaining: 3.69s\n",
            "2:\tlearn: 1.0874122\ttotal: 830ms\tremaining: 3.32s\n",
            "3:\tlearn: 1.0836067\ttotal: 1.1s\tremaining: 3.02s\n",
            "4:\tlearn: 1.0798035\ttotal: 1.36s\tremaining: 2.73s\n",
            "5:\tlearn: 1.0761241\ttotal: 1.62s\tremaining: 2.43s\n",
            "6:\tlearn: 1.0727250\ttotal: 1.89s\tremaining: 2.16s\n",
            "7:\tlearn: 1.0693649\ttotal: 2.15s\tremaining: 1.89s\n",
            "8:\tlearn: 1.0658729\ttotal: 2.42s\tremaining: 1.62s\n",
            "9:\tlearn: 1.0623679\ttotal: 2.69s\tremaining: 1.34s\n",
            "10:\tlearn: 1.0590924\ttotal: 2.99s\tremaining: 1.09s\n",
            "11:\tlearn: 1.0559910\ttotal: 3.26s\tremaining: 815ms\n",
            "12:\tlearn: 1.0529779\ttotal: 3.52s\tremaining: 542ms\n",
            "13:\tlearn: 1.0495648\ttotal: 3.79s\tremaining: 271ms\n",
            "14:\tlearn: 1.0465115\ttotal: 4.06s\tremaining: 0us\n",
            "Accuracy: 0.6305170239596469\n",
            "Most important features:\n",
            "1. ovsyanochan\n",
            "2. mudakoff\n",
            "3. styd.pozor\n",
            "4. 4ch\n",
            "5. rhymes\n",
            "6. pixel_stickers\n",
            "7. tumblr_vacuum\n",
            "8. reflexia_our_feelings\n",
            "9. pustota_diary\n",
            "10. fuck_humor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be5RUvZIYLcL"
      },
      "source": [
        "#### Пол"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2JBarQNYLcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026d660f-3e3b-44f3-d1fd-3863ddf50d31"
      },
      "source": [
        "model = CatBoostClassifier(iterations=15, learning_rate=0.01, depth=10, loss_function='MultiClass')\n",
        "model.fit(X_train, y_sex_train)\n",
        "y_pred = model.predict(X_test).reshape(-1,)\n",
        "print(\"Accuracy:\", np.mean(y_pred == y_sex_test))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(model.get_feature_importance(), features, 10)):\n",
        "    print(str(i+1) + \".\", name)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6902555\ttotal: 219ms\tremaining: 3.07s\n",
            "1:\tlearn: 0.6876998\ttotal: 419ms\tremaining: 2.73s\n",
            "2:\tlearn: 0.6851113\ttotal: 467ms\tremaining: 1.87s\n",
            "3:\tlearn: 0.6824323\ttotal: 669ms\tremaining: 1.84s\n",
            "4:\tlearn: 0.6797071\ttotal: 866ms\tremaining: 1.73s\n",
            "5:\tlearn: 0.6770563\ttotal: 1.04s\tremaining: 1.56s\n",
            "6:\tlearn: 0.6745194\ttotal: 1.23s\tremaining: 1.41s\n",
            "7:\tlearn: 0.6719190\ttotal: 1.41s\tremaining: 1.23s\n",
            "8:\tlearn: 0.6693069\ttotal: 1.58s\tremaining: 1.06s\n",
            "9:\tlearn: 0.6666918\ttotal: 1.76s\tremaining: 882ms\n",
            "10:\tlearn: 0.6641932\ttotal: 1.94s\tremaining: 705ms\n",
            "11:\tlearn: 0.6615513\ttotal: 2.12s\tremaining: 531ms\n",
            "12:\tlearn: 0.6591551\ttotal: 2.3s\tremaining: 353ms\n",
            "13:\tlearn: 0.6566015\ttotal: 2.47s\tremaining: 177ms\n",
            "14:\tlearn: 0.6541784\ttotal: 2.65s\tremaining: 0us\n",
            "Accuracy: 0.8108448928121059\n",
            "Most important features:\n",
            "1. 40kg\n",
            "2. mudakoff\n",
            "3. girlmeme\n",
            "4. igm\n",
            "5. modnailru\n",
            "6. cook_good\n",
            "7. thesmolny\n",
            "8. i_d_t\n",
            "9. reflexia_our_feelings\n",
            "10. zerofat\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}